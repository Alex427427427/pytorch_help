{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pytorch_logo.png/800px-Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to gpu_indx if GPU is avaliable\n",
    "# GPU index is set by the NVIDIA Drivers if you have only one GPU then it should always be 0\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # If you don't have a NVIDIA GPU you can just run it on your CPU\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "# This inbuilt class will download the MNIST dataset to the provided \"root\" directory\n",
    "\n",
    "# Input Parameters:\n",
    "# root: the directory the dataset is in/should be downloaded to\n",
    "# train: is this the train split or not? if not it's the test split\n",
    "# download: if it can't find the dataset in the provided directory should it download the dataset?\n",
    "# transform: the transform/augmentation to perform on the image data, the images are loaded as PIL images\n",
    "# so we need to at least convert them to tensors (Pytorch has MANY different data augmentations)\n",
    "dataset_train = MNIST(root='.', train=True,  download=True, transform=transforms.ToTensor())\n",
    "dataset_test  = MNIST(root='.', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "data_loader_train = DataLoader(dataset_train, shuffle=True, batch_size=batch_size, num_workers=0) \n",
    "data_loader_test = DataLoader(dataset_test, shuffle=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# NOTE:num_workers is the number of extra threads the dataloader will spawn to load the data from file, \n",
    "# you will rarely need more than 4 \n",
    "# NOTE!!! ON WINDOWS THERE CAN BE ISSUES WITH HAVING MORE THAN 0 WORKERS!! IF YOUR TRAINING LOOP STALLS AND DOES\n",
    "# NOTHING SET num_workers TO 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataSET size:\", len(dataset_train))\n",
    "print(\"Test DataSET size:\", len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataLOADER size:\", len(data_loader_train))\n",
    "print(\"Test DataLOADER size:\", len(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLOADER will loop through a batch of indices and provide them to the dataSET one at a time\n",
    "# and will \"batch\" together all the datapoints along the \"batch dimension\" (dim 0) returning a single tensor\n",
    "train_data_iter = iter(data_loader_train)\n",
    "data, labels = train_data_iter.next()\n",
    "print(\"Input Data shape\", data.shape)\n",
    "print(\"Target Data shape\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(data, nrow=16)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create an MLP! </h3>\n",
    "We've seen how to create our own Pytorch model using nn.Module, however up untill now we've mostly been using the inbuilt nn.Linear class (a single linear layer!). We will now (finally) create a \"Deep Learning\" model, in our case a \"Multi-Layer Perceptron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a simple 3 layer MLP network\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(SimpleMLP, self).__init__() \n",
    "        # The output of one layer should be the same size as the input of the next!\n",
    "        self.fc_1 = nn.Linear(input_size, hidden_sizes[0]) \n",
    "        self.fc_2 = nn.Linear(hidden_sizes[0], hidden_sizes[1]) \n",
    "        self.fc_3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The data we pass the model is a batch of single channel images\n",
    "        # with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        # To use it in a linear layer\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        \n",
    "        # We will use a relu activation function for this network! (F.relu)\n",
    "        # NOTE F.relu is the \"functional\" version of the activation function!\n",
    "        # nn.ReLU is a class constructor of a \"ReLU\" object\n",
    "        # These two things are the same for MOST purposes!\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        # NO activation function on the output!\n",
    "        # The loss function we will use (nn.CrossEntropyLoss) incorporates a SoftMax\n",
    "        # into the loss to and ensures stability!\n",
    "        x = self.fc_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-Class Classification </h2>\n",
    "We need to change the evaluation function as we are now performing multiclass classification! To get our \"prediction\" we are going to simply take the ouput of the model with the largest value! To do this we will take the \"argmax\" across the outputs (dim 1), this will give us the index of the largest value per-element in the batch! We don't need to perform a Softmax first as the output with the largest value will also be the one with the highest Softmax score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(data_batch, target_batch, model, optimizer, loss_func, device):\n",
    "    \n",
    "    # Ensure the data and targets are on the same device as the model!\n",
    "    data_batch = data_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    target_pred = model(data_batch)\n",
    "    loss = loss_func(target_pred, target_batch)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def eval_step(data_batch, target_batch, model, device):\n",
    "    with torch.no_grad():\n",
    "        # Ensure the data and targets are on the same device as the model!\n",
    "        data_batch = data_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        target_pred = model(data_batch)\n",
    "        # Take the argmax of the model's output\n",
    "        predictions = target_pred.argmax(1)\n",
    "\n",
    "        num_correct = (predictions == target_batch).sum()\n",
    "        \n",
    "        return num_correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calulate the accuracy of our model for the given data and labels!\n",
    "def eval_loop(eval_dataloader, model, device):\n",
    "    total_correct = 0\n",
    "\n",
    "    # All we need to do now is loop over the data loader! Much nicer! \n",
    "    for data, target in eval_dataloader:\n",
    "        num_correct = eval_step(data, target, model, device)\n",
    "        total_correct += num_correct\n",
    "        \n",
    "    return total_correct/len(eval_dataloader.dataset)\n",
    "\n",
    "\n",
    "# Function to perform our training loop\n",
    "def training_loop(train_dataloader, val_dataloader, model, optimizer, loss_func, device, num_epochs):\n",
    "    \n",
    "    loss_logger = []\n",
    "    val_accuracy_logger = []\n",
    "    train_accuracy_logger = []\n",
    "\n",
    "    # A single loop over the whole dataset is called an \"epoch\"\n",
    "    for epoch in range(num_epochs):\n",
    "        val_acc = eval_loop(val_dataloader, model, device)\n",
    "        val_accuracy_logger.append(val_acc)\n",
    "        \n",
    "        train_acc = eval_loop(train_dataloader, model, device)\n",
    "        train_accuracy_logger.append(train_acc)\n",
    "        \n",
    "        # All we need to do now is loop over the data loader! Much nicer! \n",
    "        for data, target in train_dataloader:\n",
    "            # Loop over the mini batches and perform a step of gradient decent every time!\n",
    "            loss = training_step(data, target, model, optimizer, loss_func, device)\n",
    "            loss_logger.append(loss)\n",
    "        \n",
    "    return loss_logger, train_accuracy_logger, val_accuracy_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_net = SimpleMLP(input_size=784, output_size=10, hidden_sizes=[128, 64]).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp_net.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in mlp_net.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "print(\"This model has %d Parameters!\" % (num_model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "loss_logger, train_acc, val_acc = training_loop(data_loader_train, data_loader_test, mlp_net, optimizer, \n",
    "                                                loss_function, device, num_epochs=num_epochs)\n",
    "\n",
    "print(\"Traing for %d epochs with a batch size of %d took %.2f seconds\" % \n",
    "      (num_epochs, batch_size, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "_ = plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = eval_loop(data_loader_test, mlp_net, device)\n",
    "print(\"Test Accuracy is %.2f%%\" % (100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
