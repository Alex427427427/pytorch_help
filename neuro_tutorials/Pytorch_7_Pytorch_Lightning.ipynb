{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pytorch_logo.png/800px-Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "# This inbuilt class will download the MNIST dataset to the provided \"root\" directory\n",
    "\n",
    "# Input Parameters:\n",
    "# root: the directory the dataset is in/should be downloaded to\n",
    "# train: is this the train split or not? if not it's the test split\n",
    "# download: if it can't find the dataset in the provided directory should it download the dataset?\n",
    "# transform: the transform/augmentation to perform on the image data, the images are loaded as PIL images\n",
    "# so we need to at least convert them to tensors (Pytorch has MANY different data augmentations)\n",
    "dataset_train = MNIST(root='.', train=True,  download=True, transform=transforms.ToTensor())\n",
    "dataset_test  = MNIST(root='.', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "data_loader_train = DataLoader(dataset_train, shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False) \n",
    "data_loader_test = DataLoader(dataset_test, shuffle=False, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "\n",
    "# NOTE:num_workers is the number of extra threads the dataloader will spawn to load the data from file, \n",
    "# you will rarely need more than 4 \n",
    "# NOTE!!! ON WINDOWS THERE CAN BE ISSUES WITH HAVING MORE THAN 0 WORKERS!! IF YOUR TRAINING LOOP STALLS AND DOES\n",
    "# NOTHING SET num_workers TO 0!\n",
    "\n",
    "# NOTE:pin_memory is only useful if you are training with a GPU!!!! If it is True then the GPU will pre-allocate\n",
    "# memory for the NEXT batch so the CPU-GPU transfer can be handled by the DMA controller freeing up the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataSET size:\", len(dataset_train))\n",
    "print(\"Test DataSET size:\", len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataLOADER size:\", len(data_loader_train))\n",
    "print(\"Test DataLOADER size:\", len(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLOADER will loop through a batch of indices and provide them to the dataSET one at a time\n",
    "# and will \"batch\" together all the datapoints along the \"batch dimension\" (dim 0) returning a single tensor\n",
    "train_data_iter = iter(data_loader_train)\n",
    "data, labels = train_data_iter.next()\n",
    "print(\"Input Data shape\", data.shape)\n",
    "print(\"Target Data shape\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(data, nrow=16)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(LightningModule):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, loss_func, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = loss_func\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Create the same layers as before!\n",
    "        self.fc_1 = nn.Linear(input_size, hidden_sizes[0]) \n",
    "        self.fc_2 = nn.Linear(hidden_sizes[0], hidden_sizes[1]) \n",
    "        self.fc_3 = nn.Linear(hidden_sizes[1], output_size)\n",
    "\n",
    "        # We'll use a torchmetrics \"Accuracy\" object to calculate the accuracies while we train!\n",
    "        self.test_accuracy = Accuracy()\n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass is exactly the same as before!\n",
    "        \n",
    "        # The data we pass the model is a batch of single channel images\n",
    "        # with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        # To use it in a linear layer\n",
    "        bs = x.shape[0]\n",
    "        x = x.reshape(bs, -1)\n",
    "        \n",
    "        # We will use a relu activation function for this network! (F.relu)\n",
    "        # NOTE F.relu is the \"functional\" version of the activation function!\n",
    "        # nn.ReLU is a class constructor of a \"ReLU\" object\n",
    "        # These two things are the same for MOST purposes!\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        \n",
    "        # NO activation function on the output!\n",
    "        # The loss function we will use (nn.CrossEntropyLoss) incorporates a SoftMax\n",
    "        # into the loss to and ensures stability!\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # We need to define a single training step\n",
    "        # all this function needs to do is perform a forward pass and calculate the loss\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.train_accuracy.update(preds, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        \n",
    "        preds = logits.argmax(1)\n",
    "        self.test_accuracy.update(preds, y)\n",
    "        \n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return data_loader_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return data_loader_test\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP(input_size=784, output_size=10, loss_func=nn.CrossEntropyLoss(), hidden_sizes=[128, 64])\n",
    "\n",
    "trainer = Trainer(accelerator = \"auto\",\n",
    "                  devices = 1 if torch.cuda.is_available() else None,\n",
    "                  max_epochs = num_epochs,\n",
    "                  callbacks = [TQDMProgressBar(refresh_rate=20)],\n",
    "                  logger = CSVLogger(save_dir=\"MNIST_test_1\"),)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traing_data = pd.read_csv(trainer.logger.log_dir + \"/metrics.csv\")\n",
    "traing_data.set_index(\"epoch\", inplace=True)\n",
    "traing_data = traing_data.groupby(level=0).sum().drop(\"step\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(traing_data[\"train_loss\"][:-1])\n",
    "plt.plot(traing_data[\"val_loss\"][:-1])\n",
    "\n",
    "plt.title(\"Train and Validation Loss\")\n",
    "_ = plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(traing_data[\"train_acc\"][:-1])\n",
    "plt.plot(traing_data[\"val_acc\"][:-1])\n",
    "\n",
    "plt.title(\"Train and Validation Accuracy\")\n",
    "_ = plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
